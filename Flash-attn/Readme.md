```python
import torch
import torch.nn as nn
import torch.nn.functional as F
```

# [Flash attention](https://readpaper.feishu.cn/docx/AC7JdtLrhoKpgxxSRM8cfUounsh)

Transformer ä½œä¸ºGPTç±»æ¨¡å‹çš„åŸºç¡€æ¶æ„æä¾›äº†å¼ºå¤§çš„ç‰¹å¾å¤„ç†èƒ½åŠ›ï¼Œä½†æ˜¯å¤„ç†æ›´é•¿ä¸Šä¸‹æ–‡ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºæ ¸å¿ƒçš„è‡ªæ³¨æ„åŠ›æ¨¡å—åœ¨åºåˆ—é•¿åº¦ä¸Šå…·æœ‰O(N^2)çš„æ—¶é—´å’Œå†…å­˜å¤æ‚åº¦ã€‚ ğŸ˜“
è¿™ç¯‡Flash Attentionçš„å·¥ä½œæ·±å…¥ç¡¬ä»¶ï¼Œæ–°æå‡ºäº†ä¸€ç§å…·æœ‰IOæ„ŸçŸ¥çš„ï¼Œå¿«é€Ÿçš„âš¡ï¸ï¼ŒèŠ‚çœå†…å­˜çš„ğŸ§ ï¼Œç²¾ç¡®çš„ğŸ¯æ³¨æ„åŠ›ç®—æ³•ã€‚ç›®å‰ï¼ŒFlash Attentionå·²ç»é›†æˆè‡³**torch2.0**

## æ ¸å¿ƒè¦ç‚¹
- âš¡ï¸ä¸ºä»€ä¹ˆåŠ å¿«äº†è®¡ç®—ï¼ŸFast
  - é™ä½äº†è€—æ—¶çš„HBMè®¿é—®æ¬¡æ•°ã€‚é‡‡ç”¨TilingæŠ€æœ¯åˆ†å—ä»HBMåŠ è½½æ•°æ®åˆ°SRAMè¿›è¡Œèåˆè®¡ç®—ã€‚
- ğŸ§ ä¸ºä»€ä¹ˆèŠ‚çœäº†å†…å­˜ï¼ŸMemory-Efficient
  - ä¸å†å¯¹ä¸­é—´çŸ©é˜µSï¼ŒPè¿›è¡Œå­˜å‚¨ã€‚åœ¨åå‘çš„æ—¶å€™é€šè¿‡Recomputationé‡æ–°è®¡ç®—æ¥è®¡ç®—æ¢¯åº¦ã€‚
- ğŸ¯ä¸ºä»€ä¹ˆæ˜¯ç²¾å‡†æ³¨æ„åŠ›ï¼ŸExact Attention
  - ç®—æ³•æµç¨‹åªæ˜¯åˆ†å—è®¡ç®—ï¼Œæ— è¿‘ä¼¼æ“ä½œã€‚

## Standard Attention Implementation
åœ¨æ³¨æ„åŠ›çš„ä¸€èˆ¬å®ç°ä¸­ï¼Œå¯¹$\mathbf{Q}, \mathbf{K}, \mathbf{V} \in \mathbb{R}^{N \times d}$ä¸‰ä¸ªè¾“å…¥æ‰§è¡Œä»¥ä¸‹ç®—æ³•å¾—åˆ°è¾“å‡º$\mathbf{O}$ï¼Œå…¶ä¸­softmaxè¡Œçº§åˆ«æ‰§è¡Œã€‚
$$
\mathbf{S}=\mathbf{Q K}^{\top} \in \mathbb{R}^{N \times N}, \quad \mathbf{P}=\operatorname{softmax}(\mathbf{S}) \in \mathbb{R}^{N \times N}, \quad \mathbf{O}=\mathbf{P} \mathbf{V} \in \mathbb{R}^{N \times d},
$$
åœ¨è¿™ä¸ªç®—æ³•ä¸­ï¼Œ$\mathbf{S}, \mathbf{P}$çŸ©é˜µéƒ½æ˜¯å¾ˆå¤§ï¼Œéœ€è¦åœ¨HBMä¸­å®ä¾‹åŒ–æ¥è¿›è¡Œå­˜å‚¨ï¼Œè¿™æ ·å°±ä¼šå¸¦æ¥å¾ˆå¤šHBMçš„è®¿é—®æ¬¡æ•°ï¼Œæœ€ç»ˆä½“ç°åˆ°ç®—æ³•æ—¶é—´ç«¯åˆ°ç«¯è¾ƒé•¿çš„å»¶è¿Ÿã€‚

![](./imgs/Standard-attention-implementation.png)

## softmaxæ”¹è¿›

ç†è®ºåŸºç¡€
åœ¨ä¼ ç»Ÿç®—æ³•ä¸­ï¼Œä¸€ç§æ–¹å¼æ˜¯å°†Maskå’ŒSoftMaxéƒ¨åˆ†èåˆï¼Œä»¥å‡å°‘è®¿å­˜æ¬¡æ•°ã€‚ç„¶è€Œï¼ŒFlashAttentionåˆ™æ›´åŠ æ¿€è¿›ï¼Œå®ƒå°†ä»è¾“å…¥$\mathbf{Q}, \mathbf{K}, \mathbf{V}$åˆ°è¾“å‡º$\mathbf{O}$çš„æ•´ä¸ªè¿‡ç¨‹è¿›è¡Œèåˆï¼Œä»¥é¿å…$\mathbf{S}, \mathbf{P}$çŸ©é˜µçš„å­˜å‚¨å¼€é”€ï¼Œå®ç°ç«¯åˆ°ç«¯çš„å»¶è¿Ÿç¼©å‡ã€‚ç„¶è€Œï¼Œç”±äºè¾“å…¥çš„é•¿åº¦$N$é€šå¸¸å¾ˆé•¿ï¼Œæ— æ³•å®Œå…¨å°†å®Œæ•´çš„$\mathbf{Q}, \mathbf{K}, \mathbf{V},\mathbf{O}$åŠä¸­é—´è®¡ç®—ç»“æœå­˜å‚¨åœ¨SRAMä¸­ã€‚å› æ­¤ï¼Œéœ€è¦ä¾èµ–HBMè¿›è¡Œè®¿å­˜æ“ä½œï¼Œä¸åŸå§‹è®¡ç®—å»¶è¿Ÿç›¸æ¯”æ²¡æœ‰å¤ªå¤§å·®å¼‚ï¼Œç”šè‡³ä¼šå˜æ…¢ï¼ˆæ²¡å…·ä½“æµ‹ï¼‰ã€‚
ä¸ºäº†è®©è®¡ç®—è¿‡ç¨‹çš„ç»“æœå®Œå…¨åœ¨SRAMä¸­ï¼Œæ‘†è„±å¯¹HBMçš„ä¾èµ–ï¼Œå¯ä»¥é‡‡ç”¨åˆ†ç‰‡æ“ä½œï¼Œæ¯æ¬¡è¿›è¡Œéƒ¨åˆ†è®¡ç®—ï¼Œç¡®ä¿è¿™äº›è®¡ç®—ç»“æœèƒ½åœ¨SRAMå†…è¿›è¡Œäº¤äº’ï¼Œå¾…å¾—åˆ°å¯¹åº”çš„ç»“æœåå†è¿›è¡Œè¾“å‡ºã€‚
è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæœ‰ä¸€ç‚¹éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä¹‹å‰å¯¹äºsoftmaxçš„è®¡ç®—æ˜¯ä»¥è¡Œä¸ºå•ä½çš„ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
$$
m(x):=\max _i x_i, \quad f(x):=\left[\begin{array}{lll}
e^{x_1-m(x)} & \ldots & e^{x_B-m(x)}
\end{array}\right], \quad \ell(x):=\sum_i f(x)_i, \quad \operatorname{softmax}(x):=\frac{f(x)}{\ell(x)}
$$
å½“æˆ‘ä»¬å°†è¾“å…¥è¿›è¡Œåˆ†ç‰‡åï¼Œæ— æ³•å¯¹å®Œæ•´çš„è¡Œæ•°æ®æ‰§è¡ŒSoftmaxæ“ä½œã€‚è¿™æ˜¯å› ä¸ºSoftmaxå‡½æ•°åœ¨è®¡ç®—æ—¶éœ€è¦è€ƒè™‘æ•´ä¸ªè¡Œçš„æ•°æ®ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å¦‚ä¸‹æ‰€ç¤ºæ–¹æ³•æ¥è·å¾—ä¸å®Œæ•´è¡ŒSoftmaxç›¸åŒçš„ç»“æœï¼Œè€Œæ— éœ€ä½¿ç”¨è¿‘ä¼¼æ“ä½œã€‚
$$
\begin{aligned}
& m(x)=m\left(\left[x^{(1)} x^{(2)}\right]\right)=\max \left(m\left(x^{(1)}\right), m\left(x^{(2)}\right)\right), \quad f(x)=\left[\begin{array}{ll}
e^{m\left(x^{(1)}\right)-m(x)} f\left(x^{(1)}\right) & e^{m\left(x^{(2)}\right)-m(x)} f\left(x^{(2)}\right)
\end{array}\right], \\
& \ell(x)=\ell\left(\left[x^{(1)} x^{(2)}\right]\right)=e^{m\left(x^{(1)}\right)-m(x)} \ell\left(x^{(1)}\right)+e^{m\left(x^{(2)}\right)-m(x)} \ell\left(x^{(2)}\right), \quad \operatorname{softmax}(x)=\frac{f(x)}{\ell(x)} .
\end{aligned}
$$


```python
input  = torch.randn(1, 5, 5)
output = torch.nn.functional.softmax(input, dim=-1)
print(output)
```

    tensor([[[0.0152, 0.1583, 0.3008, 0.3653, 0.1603],
             [0.2560, 0.4479, 0.0961, 0.0220, 0.1780],
             [0.1330, 0.0715, 0.5134, 0.0288, 0.2533],
             [0.0429, 0.0966, 0.6844, 0.0379, 0.1381],
             [0.0322, 0.1192, 0.3882, 0.1119, 0.3486]]])
    

* åŸå§‹çš„softmaxå…¬å¼ä¸º $$\operatorname{softmax}(\boldsymbol{x})_i=\frac{e^{\boldsymbol{x}_i}}{\sum_j e^{\boldsymbol{x}_j}}$$è¿™é‡Œåœ¨ä»£ç ä¸­å®ç°çš„æ—¶å€™é€šå¸¸ä¼šå…ˆå‡å»æœ€å¤§å€¼ï¼Œç„¶åè®¡ç®—æŒ‡æ•°ï¼Œæœ€åå†å½’ä¸€åŒ–ï¼Œè¿™æ ·åšçš„ç›®çš„æ˜¯ä¸ºäº†é˜²æ­¢æŒ‡æ•°è¿ç®—æº¢å‡ºã€‚$$\operatorname{softmax}(\boldsymbol{x})_i=\frac{e^{x_i}}{\sum_j e^{x_j}}=\frac{e^{-m}}{e^{-m}} \frac{e^{x_i}}{\sum_j e^{x_j}}=\frac{e^{x_i-m}}{\sum_j e^{x_j-m}}$$


```python
def raw_softmax(x,dim=0):
    x_max = x.max(dim=dim, keepdim=True)[0] # è®¡ç®—dimç»´åº¦çš„æœ€å¤§å€¼ maxä¼šè¿”å›æœ€å¤§å€¼å’Œæœ€å¤§å€¼çš„ä½ç½®
    x_exp = torch.exp(x - x_max) # è®¡ç®—eçš„x - x_maxæ¬¡æ–¹
    x_sum = x_exp.sum(dim=dim, keepdim=True) # è®¡ç®—dimç»´åº¦çš„å’Œ
    return x_exp / x_sum # è®¡ç®—softmax
print(raw_softmax(input,dim=-1))
```

    tensor([[[0.0152, 0.1583, 0.3008, 0.3653, 0.1603],
             [0.2560, 0.4479, 0.0961, 0.0220, 0.1780],
             [0.1330, 0.0715, 0.5134, 0.0288, 0.2533],
             [0.0429, 0.0966, 0.6844, 0.0379, 0.1381],
             [0.0322, 0.1192, 0.3882, 0.1119, 0.3486]]])
    

* åˆ†å—softmax  

![](./imgs/åˆ†å—softmax.png)


```python
def split_softmax(input, dim=0,split_size=2):
    split_inputs = torch.split(input, split_size, dim=dim)
    def softmax(x,dim=0):
        x_max = x.max(dim=dim, keepdim=True)[0]
        x_exp = torch.exp(x - x_max)
        return [x_exp , x_max]

    outputs = []
    for split_input in split_inputs:
        outputs.append(softmax(split_input,dim=dim)) # è®¡ç®—æ¯ä¸ªå—çš„softmaxåˆ†å­å’Œæœ€å¤§å€¼ 

    x_max = torch.cat([output[1] for output in outputs], dim=dim).max(dim=dim, keepdim=True)[0]  # æ‹¼æ¥æ¯ä¸ªå—çš„æœ€å¤§å€¼
    for i in range(len(outputs)):
        outputs[i][0] = outputs[i][0] / torch.exp(x_max - outputs[i][1]) # è®¡ç®—æ¯ä¸ªå—çš„softmaxå€¼
    output = torch.cat([output[0] for output in outputs], dim=dim) # æ‹¼æ¥æ¯ä¸ªå—çš„softmaxå€¼
    return output / output.sum(dim=dim, keepdim=True)
```

## Pytorch Flash Attentionä½¿ç”¨ 
è¿™é‡Œtorch2.0å·²ç»å®ç°äº†flash attention,å¯ä»¥å‚è€ƒå®˜æ–¹æ–‡æ¡£ä½¿ç”¨ï¼Œè¿™é‡Œä»¥[å®˜ç½‘](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)çš„ä¾‹å­ä¸ºä¾‹


```python
print('torch = ',torch.__version__)
!nvcc --version
```

    torch =  2.1.0
    nvcc: NVIDIA (R) Cuda compiler driver
    Copyright (c) 2005-2022 NVIDIA Corporation
    Built on Wed_Sep_21_10:41:10_Pacific_Daylight_Time_2022
    Cuda compilation tools, release 11.8, V11.8.89
    Build cuda_11.8.r11.8/compiler.31833905_0
    


```python
from torch.nn.functional import scaled_dot_product_attention
import time
# Optionally use the context manager to ensure one of the fused kernels is run
query = torch.rand(32, 8, 51200, 64, dtype=torch.float16, device="cuda")
key = torch.rand(32, 8, 51200, 64, dtype=torch.float16, device="cuda")
value = torch.rand(32, 8, 51200, 64, dtype=torch.float16, device="cuda")
```

å·®åˆ«å¾ˆéš¾æµ‹å‡ºæ¥ï¼Œå¯èƒ½éœ€è¦åœ¨æ›´å¤§çš„è®­ç»ƒä¸Šæ‰èƒ½ä½“ç°ä¼˜åŠ¿ï¼Œtorchç°åœ¨é»˜è®¤æ˜¯ä½¿ç”¨flash attentionï¼Œå¦‚æœéœ€è¦ä½¿ç”¨åŸå§‹çš„attentionï¼Œå¯ä»¥è®¾ç½®torch.backends.cuda.sdp_kernel(enable_flash=False,enable_math=True,enable_mem_efficient=True)


```python
with torch.backends.cuda.sdp_kernel(enable_flash=True,enable_math=False,enable_mem_efficient=False):
    attn_output = scaled_dot_product_attention(query, key, value)
with torch.backends.cuda.sdp_kernel(enable_flash=False,enable_math=True,enable_mem_efficient=True):
    attn_output = scaled_dot_product_attention(query, key, value)
```

    time =  0.0 ms
    torch.Size([32, 8, 51200, 64])
    time =  1000.1659393310547 ms
    torch.Size([32, 8, 51200, 64])
    
