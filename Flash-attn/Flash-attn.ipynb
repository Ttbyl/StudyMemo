{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Flash attention](https://readpaper.feishu.cn/docx/AC7JdtLrhoKpgxxSRM8cfUounsh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer ä½œä¸ºGPTç±»æ¨¡å‹çš„åŸºç¡€æ¶æ„æä¾›äº†å¼ºå¤§çš„ç‰¹å¾å¤„ç†èƒ½åŠ›ï¼Œä½†æ˜¯å¤„ç†æ›´é•¿ä¸Šä¸‹æ–‡ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºæ ¸å¿ƒçš„è‡ªæ³¨æ„åŠ›æ¨¡å—åœ¨åºåˆ—é•¿åº¦ä¸Šå…·æœ‰O(N^2)çš„æ—¶é—´å’Œå†…å­˜å¤æ‚åº¦ã€‚ ğŸ˜“\n",
    "è¿™ç¯‡Flash Attentionçš„å·¥ä½œæ·±å…¥ç¡¬ä»¶ï¼Œæ–°æå‡ºäº†ä¸€ç§å…·æœ‰IOæ„ŸçŸ¥çš„ï¼Œå¿«é€Ÿçš„âš¡ï¸ï¼ŒèŠ‚çœå†…å­˜çš„ğŸ§ ï¼Œç²¾ç¡®çš„ğŸ¯æ³¨æ„åŠ›ç®—æ³•ã€‚ç›®å‰ï¼ŒFlash Attentionå·²ç»é›†æˆè‡³**torch2.0**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ ¸å¿ƒè¦ç‚¹\n",
    "- âš¡ï¸ä¸ºä»€ä¹ˆåŠ å¿«äº†è®¡ç®—ï¼ŸFast\n",
    "  - é™ä½äº†è€—æ—¶çš„HBMè®¿é—®æ¬¡æ•°ã€‚é‡‡ç”¨TilingæŠ€æœ¯åˆ†å—ä»HBMåŠ è½½æ•°æ®åˆ°SRAMè¿›è¡Œèåˆè®¡ç®—ã€‚\n",
    "- ğŸ§ ä¸ºä»€ä¹ˆèŠ‚çœäº†å†…å­˜ï¼ŸMemory-Efficient\n",
    "  - ä¸å†å¯¹ä¸­é—´çŸ©é˜µSï¼ŒPè¿›è¡Œå­˜å‚¨ã€‚åœ¨åå‘çš„æ—¶å€™é€šè¿‡Recomputationé‡æ–°è®¡ç®—æ¥è®¡ç®—æ¢¯åº¦ã€‚\n",
    "- ğŸ¯ä¸ºä»€ä¹ˆæ˜¯ç²¾å‡†æ³¨æ„åŠ›ï¼ŸExact Attention\n",
    "  - ç®—æ³•æµç¨‹åªæ˜¯åˆ†å—è®¡ç®—ï¼Œæ— è¿‘ä¼¼æ“ä½œã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Attention Implementation\n",
    "åœ¨æ³¨æ„åŠ›çš„ä¸€èˆ¬å®ç°ä¸­ï¼Œå¯¹$\\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\in \\mathbb{R}^{N \\times d}$ä¸‰ä¸ªè¾“å…¥æ‰§è¡Œä»¥ä¸‹ç®—æ³•å¾—åˆ°è¾“å‡º$\\mathbf{O}$ï¼Œå…¶ä¸­softmaxè¡Œçº§åˆ«æ‰§è¡Œã€‚\n",
    "$$\n",
    "\\mathbf{S}=\\mathbf{Q K}^{\\top} \\in \\mathbb{R}^{N \\times N}, \\quad \\mathbf{P}=\\operatorname{softmax}(\\mathbf{S}) \\in \\mathbb{R}^{N \\times N}, \\quad \\mathbf{O}=\\mathbf{P} \\mathbf{V} \\in \\mathbb{R}^{N \\times d},\n",
    "$$\n",
    "åœ¨è¿™ä¸ªç®—æ³•ä¸­ï¼Œ$\\mathbf{S}, \\mathbf{P}$çŸ©é˜µéƒ½æ˜¯å¾ˆå¤§ï¼Œéœ€è¦åœ¨HBMä¸­å®ä¾‹åŒ–æ¥è¿›è¡Œå­˜å‚¨ï¼Œè¿™æ ·å°±ä¼šå¸¦æ¥å¾ˆå¤šHBMçš„è®¿é—®æ¬¡æ•°ï¼Œæœ€ç»ˆä½“ç°åˆ°ç®—æ³•æ—¶é—´ç«¯åˆ°ç«¯è¾ƒé•¿çš„å»¶è¿Ÿã€‚\n",
    "\n",
    "![](./imgs/Standard-attention-implementation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmaxæ”¹è¿›\n",
    "\n",
    "ç†è®ºåŸºç¡€\n",
    "åœ¨ä¼ ç»Ÿç®—æ³•ä¸­ï¼Œä¸€ç§æ–¹å¼æ˜¯å°†Maskå’ŒSoftMaxéƒ¨åˆ†èåˆï¼Œä»¥å‡å°‘è®¿å­˜æ¬¡æ•°ã€‚ç„¶è€Œï¼ŒFlashAttentionåˆ™æ›´åŠ æ¿€è¿›ï¼Œå®ƒå°†ä»è¾“å…¥$\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}$åˆ°è¾“å‡º$\\mathbf{O}$çš„æ•´ä¸ªè¿‡ç¨‹è¿›è¡Œèåˆï¼Œä»¥é¿å…$\\mathbf{S}, \\mathbf{P}$çŸ©é˜µçš„å­˜å‚¨å¼€é”€ï¼Œå®ç°ç«¯åˆ°ç«¯çš„å»¶è¿Ÿç¼©å‡ã€‚ç„¶è€Œï¼Œç”±äºè¾“å…¥çš„é•¿åº¦$N$é€šå¸¸å¾ˆé•¿ï¼Œæ— æ³•å®Œå…¨å°†å®Œæ•´çš„$\\mathbf{Q}, \\mathbf{K}, \\mathbf{V},\\mathbf{O}$åŠä¸­é—´è®¡ç®—ç»“æœå­˜å‚¨åœ¨SRAMä¸­ã€‚å› æ­¤ï¼Œéœ€è¦ä¾èµ–HBMè¿›è¡Œè®¿å­˜æ“ä½œï¼Œä¸åŸå§‹è®¡ç®—å»¶è¿Ÿç›¸æ¯”æ²¡æœ‰å¤ªå¤§å·®å¼‚ï¼Œç”šè‡³ä¼šå˜æ…¢ï¼ˆæ²¡å…·ä½“æµ‹ï¼‰ã€‚\n",
    "ä¸ºäº†è®©è®¡ç®—è¿‡ç¨‹çš„ç»“æœå®Œå…¨åœ¨SRAMä¸­ï¼Œæ‘†è„±å¯¹HBMçš„ä¾èµ–ï¼Œå¯ä»¥é‡‡ç”¨åˆ†ç‰‡æ“ä½œï¼Œæ¯æ¬¡è¿›è¡Œéƒ¨åˆ†è®¡ç®—ï¼Œç¡®ä¿è¿™äº›è®¡ç®—ç»“æœèƒ½åœ¨SRAMå†…è¿›è¡Œäº¤äº’ï¼Œå¾…å¾—åˆ°å¯¹åº”çš„ç»“æœåå†è¿›è¡Œè¾“å‡ºã€‚\n",
    "è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæœ‰ä¸€ç‚¹éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä¹‹å‰å¯¹äºsoftmaxçš„è®¡ç®—æ˜¯ä»¥è¡Œä¸ºå•ä½çš„ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š\n",
    "$$\n",
    "m(x):=\\max _i x_i, \\quad f(x):=\\left[\\begin{array}{lll}\n",
    "e^{x_1-m(x)} & \\ldots & e^{x_B-m(x)}\n",
    "\\end{array}\\right], \\quad \\ell(x):=\\sum_i f(x)_i, \\quad \\operatorname{softmax}(x):=\\frac{f(x)}{\\ell(x)}\n",
    "$$\n",
    "å½“æˆ‘ä»¬å°†è¾“å…¥è¿›è¡Œåˆ†ç‰‡åï¼Œæ— æ³•å¯¹å®Œæ•´çš„è¡Œæ•°æ®æ‰§è¡ŒSoftmaxæ“ä½œã€‚è¿™æ˜¯å› ä¸ºSoftmaxå‡½æ•°åœ¨è®¡ç®—æ—¶éœ€è¦è€ƒè™‘æ•´ä¸ªè¡Œçš„æ•°æ®ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å¦‚ä¸‹æ‰€ç¤ºæ–¹æ³•æ¥è·å¾—ä¸å®Œæ•´è¡ŒSoftmaxç›¸åŒçš„ç»“æœï¼Œè€Œæ— éœ€ä½¿ç”¨è¿‘ä¼¼æ“ä½œã€‚\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& m(x)=m\\left(\\left[x^{(1)} x^{(2)}\\right]\\right)=\\max \\left(m\\left(x^{(1)}\\right), m\\left(x^{(2)}\\right)\\right), \\quad f(x)=\\left[\\begin{array}{ll}\n",
    "e^{m\\left(x^{(1)}\\right)-m(x)} f\\left(x^{(1)}\\right) & e^{m\\left(x^{(2)}\\right)-m(x)} f\\left(x^{(2)}\\right)\n",
    "\\end{array}\\right], \\\\\n",
    "& \\ell(x)=\\ell\\left(\\left[x^{(1)} x^{(2)}\\right]\\right)=e^{m\\left(x^{(1)}\\right)-m(x)} \\ell\\left(x^{(1)}\\right)+e^{m\\left(x^{(2)}\\right)-m(x)} \\ell\\left(x^{(2)}\\right), \\quad \\operatorname{softmax}(x)=\\frac{f(x)}{\\ell(x)} .\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0152, 0.1583, 0.3008, 0.3653, 0.1603],\n",
      "         [0.2560, 0.4479, 0.0961, 0.0220, 0.1780],\n",
      "         [0.1330, 0.0715, 0.5134, 0.0288, 0.2533],\n",
      "         [0.0429, 0.0966, 0.6844, 0.0379, 0.1381],\n",
      "         [0.0322, 0.1192, 0.3882, 0.1119, 0.3486]]])\n"
     ]
    }
   ],
   "source": [
    "input  = torch.randn(1, 5, 5)\n",
    "output = torch.nn.functional.softmax(input, dim=-1)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* åŸå§‹çš„softmaxå…¬å¼ä¸º $$\\operatorname{softmax}(\\boldsymbol{x})_i=\\frac{e^{\\boldsymbol{x}_i}}{\\sum_j e^{\\boldsymbol{x}_j}}$$è¿™é‡Œåœ¨ä»£ç ä¸­å®ç°çš„æ—¶å€™é€šå¸¸ä¼šå…ˆå‡å»æœ€å¤§å€¼ï¼Œç„¶åè®¡ç®—æŒ‡æ•°ï¼Œæœ€åå†å½’ä¸€åŒ–ï¼Œè¿™æ ·åšçš„ç›®çš„æ˜¯ä¸ºäº†é˜²æ­¢æŒ‡æ•°è¿ç®—æº¢å‡ºã€‚$$\\operatorname{softmax}(\\boldsymbol{x})_i=\\frac{e^{x_i}}{\\sum_j e^{x_j}}=\\frac{e^{-m}}{e^{-m}} \\frac{e^{x_i}}{\\sum_j e^{x_j}}=\\frac{e^{x_i-m}}{\\sum_j e^{x_j-m}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0152, 0.1583, 0.3008, 0.3653, 0.1603],\n",
      "         [0.2560, 0.4479, 0.0961, 0.0220, 0.1780],\n",
      "         [0.1330, 0.0715, 0.5134, 0.0288, 0.2533],\n",
      "         [0.0429, 0.0966, 0.6844, 0.0379, 0.1381],\n",
      "         [0.0322, 0.1192, 0.3882, 0.1119, 0.3486]]])\n"
     ]
    }
   ],
   "source": [
    "def raw_softmax(x,dim=0):\n",
    "    x_max = x.max(dim=dim, keepdim=True)[0] # è®¡ç®—dimç»´åº¦çš„æœ€å¤§å€¼ maxä¼šè¿”å›æœ€å¤§å€¼å’Œæœ€å¤§å€¼çš„ä½ç½®\n",
    "    x_exp = torch.exp(x - x_max) # è®¡ç®—eçš„x - x_maxæ¬¡æ–¹\n",
    "    x_sum = x_exp.sum(dim=dim, keepdim=True) # è®¡ç®—dimç»´åº¦çš„å’Œ\n",
    "    return x_exp / x_sum # è®¡ç®—softmax\n",
    "print(raw_softmax(input,dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* åˆ†å—softmax  \n",
    "\n",
    "![](./imgs/åˆ†å—softmax.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_softmax(input, dim=0,split_size=2):\n",
    "    split_inputs = torch.split(input, split_size, dim=dim)\n",
    "    def softmax(x,dim=0):\n",
    "        x_max = x.max(dim=dim, keepdim=True)[0]\n",
    "        x_exp = torch.exp(x - x_max)\n",
    "        return [x_exp , x_max]\n",
    "\n",
    "    outputs = []\n",
    "    for split_input in split_inputs:\n",
    "        outputs.append(softmax(split_input,dim=dim)) # è®¡ç®—æ¯ä¸ªå—çš„softmaxåˆ†å­å’Œæœ€å¤§å€¼ \n",
    "\n",
    "    x_max = torch.cat([output[1] for output in outputs], dim=dim).max(dim=dim, keepdim=True)[0]  # æ‹¼æ¥æ¯ä¸ªå—çš„æœ€å¤§å€¼\n",
    "    for i in range(len(outputs)):\n",
    "        outputs[i][0] = outputs[i][0] / torch.exp(x_max - outputs[i][1]) # è®¡ç®—æ¯ä¸ªå—çš„softmaxå€¼\n",
    "    output = torch.cat([output[0] for output in outputs], dim=dim) # æ‹¼æ¥æ¯ä¸ªå—çš„softmaxå€¼\n",
    "    return output / output.sum(dim=dim, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Flash Attentionä½¿ç”¨ \n",
    "è¿™é‡Œtorch2.0å·²ç»å®ç°äº†flash attention,å¯ä»¥å‚è€ƒå®˜æ–¹æ–‡æ¡£ä½¿ç”¨ï¼Œè¿™é‡Œä»¥[å®˜ç½‘](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)çš„ä¾‹å­ä¸ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch =  2.1.0\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:41:10_Pacific_Daylight_Time_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n"
     ]
    }
   ],
   "source": [
    "print('torch = ',torch.__version__)\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import scaled_dot_product_attention\n",
    "import time\n",
    "# Optionally use the context manager to ensure one of the fused kernels is run\n",
    "query = torch.rand(32, 8, 51200, 64, dtype=torch.float16, device=\"cuda\")\n",
    "key = torch.rand(32, 8, 51200, 64, dtype=torch.float16, device=\"cuda\")\n",
    "value = torch.rand(32, 8, 51200, 64, dtype=torch.float16, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å·®åˆ«å¾ˆéš¾æµ‹å‡ºæ¥ï¼Œå¯èƒ½éœ€è¦åœ¨æ›´å¤§çš„è®­ç»ƒä¸Šæ‰èƒ½ä½“ç°ä¼˜åŠ¿ï¼Œtorchç°åœ¨é»˜è®¤æ˜¯ä½¿ç”¨flash attentionï¼Œå¦‚æœéœ€è¦ä½¿ç”¨åŸå§‹çš„attentionï¼Œå¯ä»¥è®¾ç½®torch.backends.cuda.sdp_kernel(enable_flash=False,enable_math=True,enable_mem_efficient=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time =  0.0 ms\n",
      "torch.Size([32, 8, 51200, 64])\n",
      "time =  1000.1659393310547 ms\n",
      "torch.Size([32, 8, 51200, 64])\n"
     ]
    }
   ],
   "source": [
    "with torch.backends.cuda.sdp_kernel(enable_flash=True,enable_math=False,enable_mem_efficient=False):\n",
    "    attn_output = scaled_dot_product_attention(query, key, value)\n",
    "with torch.backends.cuda.sdp_kernel(enable_flash=False,enable_math=True,enable_mem_efficient=True):\n",
    "    attn_output = scaled_dot_product_attention(query, key, value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_yue",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
