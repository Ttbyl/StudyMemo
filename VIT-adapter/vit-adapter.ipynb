{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "from ops.modules import MSDeformAttn\n",
    "from timm.layers import DropPath\n",
    "import torch.utils.checkpoint as cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [VIT-adapter](https://zhuanlan.zhihu.com/p/608272954)\n",
    "ViT-Adapter，源于一篇被ICLR 2023接受的论文，是一个面向密集型预测任务的轻量级适配器。不同于以往复杂的网络架构调整，ViT-Adapter为纯视觉Transformer（ViT）带来了革命性的转变，使其能够在无需预训练微调的情况下，直接应用于对象检测、实例分割、语义分割等任务，并达到与专门设计的模型相媲美的性能。这个项目在GitHub上提供了详尽的代码实现，以及一系列实验环境配置，助力开发者和研究者快速上手，探索视觉智能的新边疆。  \n",
    "\n",
    "***Paper***:https://arxiv.org/abs/2210.03453  \n",
    "***Github***:https://github.com/czczup/ViT-Adapter\n",
    "\n",
    "在 EVA 和 DINOv2 的下游任务中均采用了该方法  \n",
    "[DINOv2 semantic segmentation example(use ViT-adapter)](https://github.com/facebookresearch/dinov2/blob/main/notebooks/semantic_segmentation.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViT-Adapter架构\n",
    "![image.png](./imgs/vit-adapter.jpg)  \n",
    "\n",
    "在VIT架构中额外添加Adapter作为微调，主要包含模块如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Prior Module\n",
    "- ***空间先验模块(Spatial Prior Module)***: 卷积可以帮助Transformer更好地学习局部空间信息。受此启发，提出了空间先验模块（SPM），通过利用卷积Stem(参考ResNet)以及若干额外的卷积层，得到了一个具有三种分辨率（1/8、1/16、1/32）的特征金字塔。最后将这些特征图展平并拼接，得到最终的空间先验特征。  \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./imgs/Spatial_Prior_module.png\" alt=\"Spatial Prior Module\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialPriorModule(nn.Module):\n",
    "    def __init__(self, inplanes=64, embed_dim=384, with_cp=False):\n",
    "        super().__init__()\n",
    "        self.with_cp = with_cp\n",
    "\n",
    "        self.stem = nn.Sequential(*[\n",
    "            nn.Conv2d(3, inplanes, kernel_size=3,\n",
    "                      stride=2, padding=1, bias=False),\n",
    "            nn.SyncBatchNorm(inplanes),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(inplanes, inplanes, kernel_size=3,\n",
    "                      stride=1, padding=1, bias=False),\n",
    "            nn.SyncBatchNorm(inplanes),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(inplanes, inplanes, kernel_size=3,\n",
    "                      stride=1, padding=1, bias=False),\n",
    "            nn.SyncBatchNorm(inplanes),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        ])\n",
    "        self.conv2 = nn.Sequential(*[\n",
    "            nn.Conv2d(inplanes, 2 * inplanes, kernel_size=3,\n",
    "                      stride=2, padding=1, bias=False),\n",
    "            nn.SyncBatchNorm(2 * inplanes),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ])\n",
    "        self.conv3 = nn.Sequential(*[\n",
    "            nn.Conv2d(2 * inplanes, 4 * inplanes, kernel_size=3,\n",
    "                      stride=2, padding=1, bias=False),\n",
    "            nn.SyncBatchNorm(4 * inplanes),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ])\n",
    "        self.conv4 = nn.Sequential(*[\n",
    "            nn.Conv2d(4 * inplanes, 4 * inplanes, kernel_size=3,\n",
    "                      stride=2, padding=1, bias=False),\n",
    "            nn.SyncBatchNorm(4 * inplanes),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ])\n",
    "        self.fc1 = nn.Conv2d(inplanes, embed_dim,\n",
    "                             kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        self.fc2 = nn.Conv2d(2 * inplanes, embed_dim,\n",
    "                             kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        self.fc3 = nn.Conv2d(4 * inplanes, embed_dim,\n",
    "                             kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        self.fc4 = nn.Conv2d(4 * inplanes, embed_dim,\n",
    "                             kernel_size=1, stride=1, padding=0, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        def _inner_forward(x):\n",
    "            # x.shape: [batch_size, channels, height, width]: [2, 3, 384, 384]\n",
    "            # [2,3,384,384] conv1-> [2,64,192,192] conv2-> [2,64,192,192] conv3-> [2,64,192,192] maxpool-> [2, 64, 96, 96]\n",
    "            c1 = self.stem(x)  # c1.shape: [2, 64, 96, 96]\n",
    "            c2 = self.conv2(c1)  # c2.shape: [2, 128, 48, 48]\n",
    "            c3 = self.conv3(c2)  # c3.shape: [2, 256, 24, 24]\n",
    "            c4 = self.conv4(c3)  # c4.shape: [2, 256, 12, 12]\n",
    "            c1 = self.fc1(c1)  # c1.shape: [2, 768, 96, 96]\n",
    "            c2 = self.fc2(c2)  # c2.shape: [2, 768, 48, 48]   384 / 8 = 48这里是8倍下采样\n",
    "            c3 = self.fc3(c3)  # c3.shape: [2, 768, 24, 24]\n",
    "            c4 = self.fc4(c4)  # c4.shape: [2, 768, 12, 12]\n",
    "\n",
    "            bs, dim, _, _ = c1.shape\n",
    "            # c1 = c1.view(bs, dim, -1).transpose(1, 2)  # 4s\n",
    "            c2 = c2.view(bs, dim, -1).transpose(1, 2)  # 8s\n",
    "            c3 = c3.view(bs, dim, -1).transpose(1, 2)  # 16s\n",
    "            c4 = c4.view(bs, dim, -1).transpose(1, 2)  # 32s\n",
    "\n",
    "            return c1, c2, c3, c4\n",
    "\n",
    "        if self.with_cp and x.requires_grad:\n",
    "            outs = cp.checkpoint(_inner_forward, x)\n",
    "        else:\n",
    "            outs = _inner_forward(x)\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Feature Injector\n",
    "- ***空间特征注入器(Spatial Feature Injector)***: 将空间先验特征注入到Transformer的每个阶段中，将原先ViT原本的输入特征$F_{vit}^i$即$x$作为Query,$F_{sp}^i$即$c$作为Key和Value，输入到Cross-Attention中。在Injector中还设置了一个可学习的参数$\\gamma$，用于控制空间先验特征与ViT特征融合的强度。在初始化的时候设置为0，保证在训练过程中，ViT特征能够先被学习。 \n",
    "\n",
    "$$\\hat{F}_{vit}^i = F_{vit}^1 + {\\gamma}^i Attention(norm(F_{vit}^i),norm(F_{sp}^i) $$\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./imgs/Spatial_Feature_Injector.png\" alt=\"Spatial Feature Injector\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Injector(nn.Module):\n",
    "    def __init__(self, dim, num_heads=6, n_points=4, n_levels=1, deform_ratio=1.0,\n",
    "                 norm_layer=partial(nn.LayerNorm, eps=1e-6), init_values=0., with_cp=False):\n",
    "        super().__init__()\n",
    "        self.with_cp = with_cp\n",
    "        self.query_norm = norm_layer(dim)\n",
    "        self.feat_norm = norm_layer(dim)\n",
    "        self.attn = MSDeformAttn(d_model=dim, n_levels=n_levels, n_heads=num_heads,\n",
    "                                 n_points=n_points, ratio=deform_ratio)\n",
    "        self.gamma = nn.Parameter(\n",
    "            init_values * torch.ones((dim)), requires_grad=True)\n",
    "\n",
    "    def forward(self, query, reference_points, feat, spatial_shapes, level_start_index):\n",
    "\n",
    "        def _inner_forward(query, feat):\n",
    "\n",
    "            attn = self.attn(self.query_norm(query), reference_points,\n",
    "                             self.feat_norm(feat), spatial_shapes,\n",
    "                             level_start_index, None)\n",
    "            return query + self.gamma * attn\n",
    "\n",
    "        if self.with_cp and query.requires_grad:\n",
    "            query = cp.checkpoint(_inner_forward, query, feat)\n",
    "        else:\n",
    "            query = _inner_forward(query, feat)\n",
    "\n",
    "        return query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-scale Feature Extractor\n",
    "- ***多尺度特征提取器(Multi-scale Feature Extractor)***: 主要由Cross-Attention和FFN组成，用于提取多尺度特征。这里的输入与Injector相反，将$F_{sp}^i$作为Query,$F_{vit}^i$作为Key和Value，输入到Cross-Attention中。这里就没有设置一个可学习的参数$\\gamma$，因为空间先验特征已经通过Injector被注入到ViT中，所以这里只需要提取多尺度特征即可。$$\\hat{F}_{sp}^i = F_{sp}^1 + Attention(norm(F_{sp}^i),norm(F_{vit}^{i+1}) $$ 这里的$F_{vit}^{i+1}$和Injector相比经过了多一个ViT Block，所以这里是*i+1*。$$F_{sp}^i = \\hat{F}_{sp}^i + FFN(norm(\\hat{F}_{sp}^i))$$\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./imgs/Multi-scale_Feature_Extractor.png\" alt=\"Multi-scale Feature Extractor\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extractor(nn.Module):\n",
    "    def __init__(self, dim, num_heads=6, n_points=4, n_levels=1, deform_ratio=1.0,\n",
    "                 with_cffn=True, cffn_ratio=0.25, drop=0., drop_path=0.,\n",
    "                 norm_layer=partial(nn.LayerNorm, eps=1e-6), with_cp=False):\n",
    "        super().__init__()\n",
    "        self.query_norm = norm_layer(dim)\n",
    "        self.feat_norm = norm_layer(dim)\n",
    "        self.attn = MSDeformAttn(d_model=dim, n_levels=n_levels, n_heads=num_heads,\n",
    "                                 n_points=n_points, ratio=deform_ratio)\n",
    "        self.with_cffn = with_cffn\n",
    "        self.with_cp = with_cp\n",
    "        if with_cffn:\n",
    "            self.ffn = ConvFFN(in_features=dim, hidden_features=int(\n",
    "                dim * cffn_ratio), drop=drop)\n",
    "            self.ffn_norm = norm_layer(dim)\n",
    "            self.drop_path = DropPath(\n",
    "                drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, query, reference_points, feat, spatial_shapes, level_start_index, H, W):\n",
    "\n",
    "        def _inner_forward(query, feat):\n",
    "\n",
    "            attn = self.attn(self.query_norm(query), reference_points,\n",
    "                             self.feat_norm(feat), spatial_shapes,\n",
    "                             level_start_index, None)\n",
    "            query = query + attn  # 这里和Injector少了gamma可学γ\n",
    "\n",
    "            if self.with_cffn:  # 这里和Injector多了cffn\n",
    "                query = query + \\\n",
    "                    self.drop_path(self.ffn(self.ffn_norm(query), H, W))\n",
    "            return query\n",
    "\n",
    "        if self.with_cp and query.requires_grad:\n",
    "            query = cp.checkpoint(_inner_forward, query, feat)\n",
    "        else:\n",
    "            query = _inner_forward(query, feat)\n",
    "\n",
    "        return query\n",
    "\n",
    "# FFN\n",
    "class ConvFFN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None,\n",
    "                 act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.dwconv = DWConv(hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dwconv(x, H, W)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DWConv(nn.Module):\n",
    "    def __init__(self, dim=768):\n",
    "        super().__init__()\n",
    "        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        B, N, C = x.shape\n",
    "        n = N // 21\n",
    "        x1 = x[:, 0:16 * n,\n",
    "               :].transpose(1, 2).view(B, C, H * 2, W * 2).contiguous()\n",
    "        x2 = x[:, 16 * n:20 * n,\n",
    "               :].transpose(1, 2).view(B, C, H, W).contiguous()\n",
    "        x3 = x[:, 20 * n:, :].transpose(1, 2).view(B,\n",
    "                                                   C, H // 2, W // 2).contiguous()\n",
    "        x1 = self.dwconv(x1).flatten(2).transpose(1, 2)\n",
    "        x2 = self.dwconv(x2).flatten(2).transpose(1, 2)\n",
    "        x3 = self.dwconv(x3).flatten(2).transpose(1, 2)\n",
    "        x = torch.cat([x1, x2, x3], dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其他注意点\n",
    "这里的代码片段是ViTAdapter模型的整体，它继承自TIMMVisionTransformer。ViTAdapter模型在原始ViT模型的基础上增加了一些新的模块，如SpatialPriorModule、InteractionBlock和deform_inputs。这些模块用于实现模型的自适应能力，以适应不同的任务和数据集。  \n",
    "模型参数如下：\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./imgs/config_vit-adapater.png\" alt=\"Multi-scale Feature Extractor\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的ViT-Adapter中的注意力模块是可以改变的，作者也在这上面做了一些消融实验。\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./imgs/Ablation_different_attention.png\" alt=\"Multi-scale Feature Extractor\"/>\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./imgs/ViT_ViT-adapter_feature.png\" alt=\"Multi-scale Feature Extractor\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViTAdapter(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath(drop_prob=0.027)\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath(drop_prob=0.055)\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath(drop_prob=0.082)\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath(drop_prob=0.109)\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath(drop_prob=0.136)\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath(drop_prob=0.164)\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath(drop_prob=0.191)\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath(drop_prob=0.218)\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath(drop_prob=0.245)\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath(drop_prob=0.273)\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath(drop_prob=0.300)\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (spm): SpatialPriorModule(\n",
      "    (stem): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (7): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): ReLU(inplace=True)\n",
      "      (9): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (conv2): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (conv3): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (conv4): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (fc1): Conv2d(64, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fc2): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fc3): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fc4): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (interactions): Sequential(\n",
      "    (0): InteractionBlock(\n",
      "      (injector): Injector(\n",
      "        (query_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (feat_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MSDeformAttn(\n",
      "          (sampling_offsets): Linear(in_features=768, out_features=288, bias=True)\n",
      "          (attention_weights): Linear(in_features=768, out_features=144, bias=True)\n",
      "          (value_proj): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (output_proj): Linear(in_features=384, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (extractor): Extractor(\n",
      "        (query_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (feat_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MSDeformAttn(\n",
      "          (sampling_offsets): Linear(in_features=768, out_features=96, bias=True)\n",
      "          (attention_weights): Linear(in_features=768, out_features=48, bias=True)\n",
      "          (value_proj): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (output_proj): Linear(in_features=384, out_features=768, bias=True)\n",
      "        )\n",
      "        (ffn): ConvFFN(\n",
      "          (fc1): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
      "          )\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop_path): DropPath(drop_prob=0.300)\n",
      "      )\n",
      "    )\n",
      "    (1): InteractionBlock(\n",
      "      (injector): Injector(\n",
      "        (query_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (feat_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MSDeformAttn(\n",
      "          (sampling_offsets): Linear(in_features=768, out_features=288, bias=True)\n",
      "          (attention_weights): Linear(in_features=768, out_features=144, bias=True)\n",
      "          (value_proj): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (output_proj): Linear(in_features=384, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (extractor): Extractor(\n",
      "        (query_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (feat_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MSDeformAttn(\n",
      "          (sampling_offsets): Linear(in_features=768, out_features=96, bias=True)\n",
      "          (attention_weights): Linear(in_features=768, out_features=48, bias=True)\n",
      "          (value_proj): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (output_proj): Linear(in_features=384, out_features=768, bias=True)\n",
      "        )\n",
      "        (ffn): ConvFFN(\n",
      "          (fc1): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
      "          )\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop_path): DropPath(drop_prob=0.300)\n",
      "      )\n",
      "    )\n",
      "    (2): InteractionBlock(\n",
      "      (injector): Injector(\n",
      "        (query_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (feat_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MSDeformAttn(\n",
      "          (sampling_offsets): Linear(in_features=768, out_features=288, bias=True)\n",
      "          (attention_weights): Linear(in_features=768, out_features=144, bias=True)\n",
      "          (value_proj): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (output_proj): Linear(in_features=384, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (extractor): Extractor(\n",
      "        (query_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (feat_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MSDeformAttn(\n",
      "          (sampling_offsets): Linear(in_features=768, out_features=96, bias=True)\n",
      "          (attention_weights): Linear(in_features=768, out_features=48, bias=True)\n",
      "          (value_proj): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (output_proj): Linear(in_features=384, out_features=768, bias=True)\n",
      "        )\n",
      "        (ffn): ConvFFN(\n",
      "          (fc1): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
      "          )\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop_path): DropPath(drop_prob=0.300)\n",
      "      )\n",
      "    )\n",
      "    (3): InteractionBlock(\n",
      "      (injector): Injector(\n",
      "        (query_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (feat_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MSDeformAttn(\n",
      "          (sampling_offsets): Linear(in_features=768, out_features=288, bias=True)\n",
      "          (attention_weights): Linear(in_features=768, out_features=144, bias=True)\n",
      "          (value_proj): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (output_proj): Linear(in_features=384, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (extractor): Extractor(\n",
      "        (query_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (feat_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MSDeformAttn(\n",
      "          (sampling_offsets): Linear(in_features=768, out_features=96, bias=True)\n",
      "          (attention_weights): Linear(in_features=768, out_features=48, bias=True)\n",
      "          (value_proj): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (output_proj): Linear(in_features=384, out_features=768, bias=True)\n",
      "        )\n",
      "        (ffn): ConvFFN(\n",
      "          (fc1): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
      "          )\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop_path): DropPath(drop_prob=0.300)\n",
      "      )\n",
      "      (extra_extractors): Sequential(\n",
      "        (0): Extractor(\n",
      "          (query_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (feat_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=768, out_features=96, bias=True)\n",
      "            (attention_weights): Linear(in_features=768, out_features=48, bias=True)\n",
      "            (value_proj): Linear(in_features=768, out_features=384, bias=True)\n",
      "            (output_proj): Linear(in_features=384, out_features=768, bias=True)\n",
      "          )\n",
      "          (ffn): ConvFFN(\n",
      "            (fc1): Linear(in_features=768, out_features=192, bias=True)\n",
      "            (dwconv): DWConv(\n",
      "              (dwconv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "            (fc2): Linear(in_features=192, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (drop_path): DropPath(drop_prob=0.300)\n",
      "        )\n",
      "        (1): Extractor(\n",
      "          (query_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (feat_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): MSDeformAttn(\n",
      "            (sampling_offsets): Linear(in_features=768, out_features=96, bias=True)\n",
      "            (attention_weights): Linear(in_features=768, out_features=48, bias=True)\n",
      "            (value_proj): Linear(in_features=768, out_features=384, bias=True)\n",
      "            (output_proj): Linear(in_features=384, out_features=768, bias=True)\n",
      "          )\n",
      "          (ffn): ConvFFN(\n",
      "            (fc1): Linear(in_features=768, out_features=192, bias=True)\n",
      "            (dwconv): DWConv(\n",
      "              (dwconv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "            (fc2): Linear(in_features=192, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (drop_path): DropPath(drop_prob=0.300)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (up): ConvTranspose2d(768, 768, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (norm1): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (norm2): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (norm3): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (norm4): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "4\n",
      "torch.Size([2, 768, 96, 96])\n",
      "torch.Size([2, 768, 48, 48])\n",
      "torch.Size([2, 768, 24, 24])\n",
      "torch.Size([2, 768, 12, 12])\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) Shanghai AI Lab. All rights reserved.\n",
    "import logging\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from mmseg.models.builder import BACKBONES\n",
    "from ops.modules import MSDeformAttn\n",
    "from timm.layers import trunc_normal_\n",
    "from torch.nn.init import normal_\n",
    "\n",
    "from base.vit import TIMMVisionTransformer\n",
    "from adapter_modules import SpatialPriorModule, InteractionBlock, deform_inputs\n",
    "\n",
    "_logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# @BACKBONES.register_module()\n",
    "class ViTAdapter(TIMMVisionTransformer):\n",
    "    def __init__(self, pretrain_size=224, num_heads=12, conv_inplane=64, n_points=4,\n",
    "                 deform_num_heads=6, init_values=0., interaction_indexes=None, with_cffn=True,\n",
    "                 cffn_ratio=0.25, deform_ratio=1.0, add_vit_feature=True, pretrained=None,\n",
    "                 use_extra_extractor=True, with_cp=False, *args, **kwargs):\n",
    "\n",
    "        super().__init__(num_heads=num_heads, pretrained=pretrained,\n",
    "                         with_cp=with_cp, *args, **kwargs)\n",
    "\n",
    "        # self.num_classes = 80\n",
    "        self.cls_token = None\n",
    "        # 获取block的个数\n",
    "        self.num_block = len(self.blocks) \n",
    "        self.pretrain_size = (pretrain_size, pretrain_size) \n",
    "        # interaction_indexes: [[0, 2], [3, 5], [6, 8], [9, 11]]\n",
    "        self.interaction_indexes = interaction_indexes\n",
    "        self.add_vit_feature = add_vit_feature\n",
    "        embed_dim = self.embed_dim # 768\n",
    "\n",
    "        # level_embed: 3 * 768\n",
    "        self.level_embed = nn.Parameter(torch.zeros(3, embed_dim))\n",
    "\n",
    "        # inplanes: 64 embed_dim: 768 \n",
    "        self.spm = SpatialPriorModule(\n",
    "            inplanes=conv_inplane, embed_dim=embed_dim, with_cp=False)\n",
    "        \n",
    "        self.interactions = nn.Sequential(*[\n",
    "            InteractionBlock(dim=embed_dim, num_heads=deform_num_heads, n_points=n_points,\n",
    "                             init_values=init_values, drop_path=self.drop_path_rate,\n",
    "                             norm_layer=self.norm_layer, with_cffn=with_cffn,\n",
    "                             cffn_ratio=cffn_ratio, deform_ratio=deform_ratio,\n",
    "                             extra_extractor=((True if i == len(interaction_indexes) - 1\n",
    "                                               else False) and use_extra_extractor),\n",
    "                             with_cp=with_cp)\n",
    "            for i in range(len(interaction_indexes))\n",
    "        ])\n",
    "        self.up = nn.ConvTranspose2d(embed_dim, embed_dim, 2, 2)\n",
    "        self.norm1 = nn.SyncBatchNorm(embed_dim)\n",
    "        self.norm2 = nn.SyncBatchNorm(embed_dim)\n",
    "        self.norm3 = nn.SyncBatchNorm(embed_dim)\n",
    "        self.norm4 = nn.SyncBatchNorm(embed_dim)\n",
    "\n",
    "        self.up.apply(self._init_weights)\n",
    "        self.spm.apply(self._init_weights)\n",
    "        self.interactions.apply(self._init_weights)\n",
    "        self.apply(self._init_deform_weights)\n",
    "        normal_(self.level_embed)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm) or isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _get_pos_embed(self, pos_embed, H, W):\n",
    "        pos_embed = pos_embed.reshape(\n",
    "            1, self.pretrain_size[0] // 16, self.pretrain_size[1] // 16, -1).permute(0, 3, 1, 2)  # [1, 576, 768] -> [1, 24, 24, 768] -> [1, 768, 24, 24]\n",
    "        pos_embed = F.interpolate(pos_embed, size=(H, W), mode='bicubic', align_corners=False).\\\n",
    "            reshape(1, -1, H * W).permute(0, 2, 1)\n",
    "        return pos_embed\n",
    "\n",
    "    def _init_deform_weights(self, m):\n",
    "        if isinstance(m, MSDeformAttn):\n",
    "            m._reset_parameters()\n",
    "\n",
    "    def _add_level_embed(self, c2, c3, c4):\n",
    "        c2 = c2 + self.level_embed[0]  # 广播相加 [2, 2304, 768] + [768]\n",
    "        c3 = c3 + self.level_embed[1]\n",
    "        c4 = c4 + self.level_embed[2]\n",
    "        return c2, c3, c4\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [2, 3, 384, 384]\n",
    "        # len(deform_input1): 3 / len(deform_input2): 3\n",
    "        # deform_inputs1[0].shape: [1, 576, 1, 2] / deform_inputs2[0].shape: [1, 3024, 1, 2]\n",
    "        # deform_inputs1[1].shape: [3, 2]         / deform_inputs2[1].shape: [1, 2]\n",
    "        # deform_inputs1[2].shape: [3]            / deform_inputs2[2].shape: [1]\n",
    "        deform_inputs1, deform_inputs2 = deform_inputs(x)\n",
    "\n",
    "        # SPM forward\n",
    "        # c1.shape: [2, 768, 96, 96] tips:96*96=9216 \n",
    "        # c2.shape: [2, 2304, 768]   tips: 96*96=9216 9216/4=2304\n",
    "        # c3.shape: [2, 576, 768]    tips:  2304/4=576\n",
    "        # c4.shape: [2, 144, 768]\n",
    "        c1, c2, c3, c4 = self.spm(x)\n",
    "        c2, c3, c4 = self._add_level_embed(c2, c3, c4)\n",
    "        # 三种分辨率（1/8、1/16、1/32）的特征金字塔\n",
    "\n",
    "        # c.shape: [2, 3024, 768]\n",
    "        c = torch.cat([c2, c3, c4], dim=1)  # 拼接:引入局部空间信息的同时，可以避免改变ViT 的原始结构\n",
    "\n",
    "        # Patch Embedding forward\n",
    "        # x.shape: [2, 576, 768] H: 24 W: 24 tips: 384/16=24 24*24=576\n",
    "        x, H, W = self.patch_embed(x)\n",
    "        bs, n, dim = x.shape\n",
    "        pos_embed = self._get_pos_embed(self.pos_embed[:, 1:], H, W) # 重采样保证一致，这里的重采样是插值\n",
    "        x = self.pos_drop(x + pos_embed)\n",
    "\n",
    "        # Interaction\n",
    "        outs = list()\n",
    "        for i, layer in enumerate(self.interactions):\n",
    "            # interations: injector +  extractor\n",
    "            # layer就是InterationBlock\n",
    "            indexes = self.interaction_indexes[i]\n",
    "            x, c = layer(x, c, self.blocks[indexes[0]:indexes[-1] + 1],\n",
    "                         deform_inputs1, deform_inputs2, H, W)\n",
    "            outs.append(x.transpose(1, 2).view(bs, dim, H, W).contiguous())\n",
    "\n",
    "        # Split & Reshape\n",
    "        c2 = c[:, 0:c2.size(1), :]\n",
    "        c3 = c[:, c2.size(1):c2.size(1) + c3.size(1), :]\n",
    "        c4 = c[:, c2.size(1) + c3.size(1):, :]\n",
    "\n",
    "        c2 = c2.transpose(1, 2).view(bs, dim, H * 2, W * 2).contiguous() # [2, 2304, 768] -> [2, 768, 48, 48]\n",
    "        c3 = c3.transpose(1, 2).view(bs, dim, H, W).contiguous() # [2, 576, 768] -> [2, 768, 24, 24]\n",
    "        c4 = c4.transpose(1, 2).view(bs, dim, H // 2, W // 2).contiguous() # [2, 144, 768] -> [2, 768, 12, 12]\n",
    "        c1 = self.up(c2) + c1 # 上采样求和\n",
    "\n",
    "        if self.add_vit_feature:\n",
    "            x1, x2, x3, x4 = outs\n",
    "            x1 = F.interpolate(x1, scale_factor=4,\n",
    "                               mode='bilinear', align_corners=False)  # 上采样4倍得到 [2, 768, 24, 24] -> [2, 768, 96, 96]\n",
    "            x2 = F.interpolate(x2, scale_factor=2,\n",
    "                               mode='bilinear', align_corners=False) # 上采样2倍得到 [2, 768, 24, 24] -> [2, 768, 48, 48]\n",
    "            x4 = F.interpolate(x4, scale_factor=0.5,\n",
    "                               mode='bilinear', align_corners=False) # 下采样2倍得到 [2, 768, 24, 24] -> [2, 768, 12, 12]\n",
    "            c1, c2, c3, c4 = c1 + x1, c2 + x2, c3 + x3, c4 + x4\n",
    "\n",
    "        # Final Norm\n",
    "        f1 = self.norm1(c1)\n",
    "        f2 = self.norm2(c2)\n",
    "        f3 = self.norm3(c3)\n",
    "        f4 = self.norm4(c4)\n",
    "        return [f1, f2, f3, f4]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = ViTAdapter(img_size=384,\n",
    "                       pretrain_size=384,\n",
    "                       patch_size=16,\n",
    "                       embed_dim=768,\n",
    "                       depth=12, # ViT中block深度，这里和下面interaction_blocks须保持一致数量\n",
    "                       num_heads=12, # ViT中多头注意力机制\n",
    "                       mlp_ratio=4, # ViT中MLP隐藏层倍数\n",
    "                       drop_path_rate=0.3, # ViT中drop_path\n",
    "                       conv_inplane=64,  # SpatialPriorModule中间conv卷积层通道数（最后会映射回dim）\n",
    "                       n_points=4,  # MSDeformAttn所需参数\n",
    "                       deform_num_heads=12,  # MSDeformAttn所需参数\n",
    "                       cffn_ratio=0.25,  # Extractor中隐藏层的通道倍数\n",
    "                       deform_ratio=0.5,  # MSDeformAttn所需参数\n",
    "                       interaction_indexes=[[0, 2], [3, 5], [6, 8], [9, 11]], # VIT中block的间隔\n",
    "                       window_attn=[False] * 12, # VIT中block是否使用window\n",
    "                       window_size=[None] * 12)\n",
    "    print(model)\n",
    "    input = torch.randn(2, 3, 384, 384)\n",
    "    model = model.cuda()\n",
    "    output = model(input.cuda())\n",
    "    print(len(output)) # 4\n",
    "    print(output[0].shape)  # torch.Size([2, 768, 96, 96])\n",
    "    print(output[1].shape)  # torch.Size([2, 768, 48, 48])\n",
    "    print(output[2].shape)  # torch.Size([2, 768, 48, 48])\n",
    "    print(output[3].shape)  # torch.Size([2, 768, 48, 48])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_yue",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
