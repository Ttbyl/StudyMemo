{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [VIT结构解析](https://blog.csdn.net/qq_37541097/article/details/118242600)\n",
    "## 1. VIT模型结构\n",
    "![](./imgs/VIT.png)  \n",
    "VIT模型结构图如上所示，VIT模型由以下几个部分组成：\n",
    "- **Patch Embedding**：将输入图像划分为固定大小的patch，然后将每个patch展平成一维向量，作为模型的输入。\n",
    "- **Transformer Encoder**：将展平后的向量输入到Transformer Encoder中，进行特征提取和变换。\n",
    "- **MLP Head**：将Transformer Encoder的输出通过一个全连接层进行分类。\n",
    "![](./imgs/VIT-B.png)\n",
    "## 2. Patch Embedding\n",
    "Patch Embedding是将输入图像划分为固定大小的patch，然后将每个patch展平成一维向量，作为模型的输入。具体步骤如下：\n",
    "- 将输入图像划分为固定大小的patch，例如16x16的patch。\n",
    "- 将每个patch展平成一维向量，例如展平后的向量维度为256。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以VIT Base为例\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    2D Image 2 Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768, norm_layer=None):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        # 用卷积来表示Patch Embedding,卷积核大小为patch_size,步长为patch_size,输出通道数为embed_dim\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size) \n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        assert H == self.img_size and W == self.img_size, \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size}*{self.img_size}).\"\n",
    "        x = self.proj(x)  # (B, embed_dim, H//patch_size, W//patch_size)\n",
    "        x = x.flatten(2).transpose(1, 2)  # (B, num_patches, embed_dim)\n",
    "        x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transformer Encoder\n",
    "Transformer Encoder是由多个Encoder Block堆叠而成，每个Encoder Block包含两个子层：Multi-Head Attention和MLP。具体步骤如下：\n",
    "- **Multi-Head Attention**：将输入的向量通过线性变换得到Query、Key、Value，然后计算注意力分数，最后将Query、Key、Value进行加权求和得到输出。\n",
    "\n",
    "![](./imgs/Multi-Head-Attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.0):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv_proj(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)  # (3, B, num_heads, N, head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # (B, num_heads, N, head_dim)\n",
    "\n",
    "        # Scaled Dot-Product Attention \n",
    "        attn = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)  # q:(B, num_heads, N, head_dim), k:(B, num_heads, head_dim, N) -> (B, num_heads, N, N)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        # attn:(B, num_heads, N, N) v:(B, num_heads, N, head_dim) -> (B, num_heads, N, head_dim) ->  (B, N, num_heads, head_dim) -> (B, N, C)\n",
    "        x = self.out_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **MLP**：将输入的向量通过两个全连接层得到输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.0):\n",
    "        super(MLP, self).__init__()\n",
    "        out_features = out_features or in_features # 如果有传入则为传入的值，否则为in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Encoder Block**：Transformer Encoder Block由两个部分组成，一个是Multi-Head Attention，另一个是MLP。在论文中，Multi-Head Attention后面接了一个Layer Normalization，然后MLP后面接了一个Layer Normalization，最后再接一个残差连接。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, drop=0.0, act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.norm1 = norm_layer(embed_dim)\n",
    "        self.attn = MultiHeadAttention(embed_dim, num_heads, drop)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "        self.norm2 = norm_layer(embed_dim)\n",
    "        self.mlp = MLP(embed_dim, hidden_features=4*embed_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop(self.attn(self.norm1(x)))  # Multi-Head Attention\n",
    "        x = x + self.drop(self.mlp(self.norm2(x)))  # MLP\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ViT模型搭建\n",
    "ViT模型由Patch Embedding、Transformer Encoder和MLP Head组成。具体步骤如下：\n",
    "![](./imgs/VIT-B.png)\n",
    "- **Patch Embedding**：将输入的图像通过卷积层得到Patch，然后通过线性层得到Patch Embedding。\n",
    "- **Transformer Encoder**：将Patch Embedding输入到Transformer Encoder中，得到输出。\n",
    "- **MLP Head**：将Transformer Encoder的输出通过一个线性层得到最终的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[nn.init.trunc_normal_讲解](https://blog.csdn.net/weixin_43135178/article/details/120622761)  \n",
    "[dropout/droppath讲解](https://blog.csdn.net/qq_43135204/article/details/127912029?spm=1001.2101.3001.6661.1&utm_medium=distribute.pc_relevant_t0.none-task-blog-2~default~BlogCommendFromBaidu~Rate-1-127912029-blog-138034012.235%5Ev43%5Epc_blog_bottom_relevance_base9&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2~default~BlogCommendFromBaidu~Rate-1-127912029-blog-138034012.235%5Ev43%5Epc_blog_bottom_relevance_base9&utm_relevant_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, num_heads=12, drop=0.0, norm_layer=nn.LayerNorm, representation_size=None):\n",
    "        super(ViT, self).__init__()\n",
    "        super(ViT, self).__init__()\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_chans, embed_dim) # Patch Embedding\n",
    "        self.cls = nn.Parameter(torch.zeros(1, 1, embed_dim)) # cls token (1, 1, embed_dim)\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.patch_embed.num_patches+1, embed_dim)) # position embedding (1, num_patches+1, embed_dim)\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "\n",
    "        self.encoder = nn.Sequential(*[EncoderBlock(embed_dim, num_heads, drop) for _ in range(12)]) # Transformer Encoder\n",
    "\n",
    "        # 如果使用nn.Modulelist，则不需要*，直接nn.Modulelist([EncoderBlock(embed_dim, num_heads, drop) for _ in range(12)])\n",
    "        # 但在forward中需要self.encoder[0](x)来调用，而不是self.encoder(x)\n",
    "            # def forward(self, x):\n",
    "            # for block in self.blocks:\n",
    "            #     x = block(x)\n",
    "            # return x\n",
    "        self.norm = norm_layer(embed_dim) # Layer Normalization\n",
    "        self.MLP = nn.Linear(embed_dim, num_classes) # MLP Head\n",
    "\n",
    "        # Initialize weights\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=.02)  # 截断正态分布\n",
    "        nn.init.trunc_normal_(self.cls, std=.02)\n",
    "        self.apply(_init_vit_weights)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.patch_embed(x) # (B, N=num_patches, C=embed_dim)\n",
    "        x = torch.cat([self.cls.expand(B, -1, -1), x], dim=1) # (B, N+1, C)\n",
    "        x = x + self.pos_embed # (B, N+1, C)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.encoder(x) # (B, N+1, C)\n",
    "        x = self.norm(x) # (B, N+1, C)\n",
    "        x = self.MLP(x[:, 0]) # (B, num_classes)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def _init_vit_weights(m):\n",
    "    \"\"\"\n",
    "    ViT weight initialization\n",
    "    :param m: module\n",
    "    \"\"\"\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.trunc_normal_(m.weight, std=.01)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.LayerNorm):\n",
    "        nn.init.zeros_(m.bias)\n",
    "        nn.init.ones_(m.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "model = ViT()\n",
    "input = torch.randn(1, 3, 224, 224)\n",
    "output = model(input)\n",
    "print(output.shape) # torch.Size([1, 1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "ViT模型搭建参数\n",
    "在论文的Table1中有给出三个模型（Base/ Large/ Huge）的参数，在源码中除了有Patch Size为16x16的外还有32x32的。其中的Layers就是Transformer Encoder中重复堆叠Encoder Block的次数，Hidden Size就是对应通过Embedding层后每个token的dim（向量的长度），MLP size是Transformer Encoder中MLP Block第一个全连接的节点个数（是Hidden Size的四倍），Heads代表Transformer中Multi-Head Attention的heads数。\n",
    "\n",
    "| Model     | Patch Size | Layers | Hidden Size D | MLP Size | Heads | Params |\n",
    "|:---------:|:----------:|:------:|:-------------:|:--------:|:-----:|:------:|\n",
    "| ViT-Base  | 16x16      | 12     | 768           | 3072     | 12    | 86M    |\n",
    "| ViT-Large | 16x16      | 24     | 1024          | 4096     | 16    | 307M   |\n",
    "| ViT-Huge  | 14x14      | 32     | 1280          | 5120     | 16    | 632M   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_yue",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
